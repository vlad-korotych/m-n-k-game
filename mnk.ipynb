{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import HBox, VBox, Label, Layout, Button, Output\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import math\n",
    "import copy\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mark(Enum):\n",
    "    NO = 0\n",
    "    X = 1\n",
    "    O = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class View(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JupyterView(View):\n",
    "    def __init__(self):\n",
    "        self.out = Output()\n",
    "    \n",
    "    def on_click(self, row, column, button):\n",
    "        self.disable_all_buttons()\n",
    "        self.turn_callback(row, column)\n",
    "    \n",
    "    def update(self, state):\n",
    "        label = Label()\n",
    "        if state.is_game_end:\n",
    "            if state.winner == Mark.NO:\n",
    "                label.value = \"Draw!\"\n",
    "            else:\n",
    "                label.value = state.winner.name + ' wins!'\n",
    "        else:\n",
    "            label.value = state.current_player.name + \"'s turn\"\n",
    "        \n",
    "        h, w = state.board.shape\n",
    "        self.buttons = []\n",
    "        buttons = []\n",
    "        \n",
    "        for r in range(h):\n",
    "            line = []\n",
    "            for c in range(w):\n",
    "                button = Button(\n",
    "                    description='',\n",
    "                    layout=Layout(width='20px', height='20px', padding='0px')\n",
    "                )\n",
    "                \n",
    "                button.on_click(partial(self.on_click, r, c))\n",
    "\n",
    "                if state.board[r][c] == Mark.X.value:\n",
    "                    button.description = Mark.X.name\n",
    "                    button.disabled = True\n",
    "                elif state.board[r][c] == Mark.O.value:\n",
    "                    button.description = Mark.O.name\n",
    "                    button.disabled = True\n",
    "                elif state.is_game_end:\n",
    "                    button.disabled = True\n",
    "                \n",
    "                line.append(button)\n",
    "                self.buttons.append(button)\n",
    "                \n",
    "            buttons.append(HBox(line))\n",
    "            \n",
    "        self.out.clear_output()\n",
    "        with self.out:\n",
    "            display(VBox([label, VBox(buttons)]))\n",
    "        \n",
    "    def disable_all_buttons(self):\n",
    "        for b in self.buttons:\n",
    "            b.disabled = True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def win(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JupyterHumanAgent(Agent):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return None\n",
    "    \n",
    "    def win(self, state):\n",
    "        pass\n",
    "    \n",
    "    def loss(self, state):\n",
    "        pass\n",
    "    \n",
    "    def draw(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Player:\n",
    "    mark: Mark\n",
    "    agent: Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GameState:\n",
    "    board: np.ndarray\n",
    "    current_player: Mark\n",
    "    is_game_end: bool\n",
    "    winner: Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, player1, player2, m=None, n=None, k=5, view=None, play=True):\n",
    "        self.width = m\n",
    "        self.height = n\n",
    "        self.row = k # win row\n",
    "       \n",
    "        width = m\n",
    "        height = n\n",
    "        \n",
    "        auto_first_turn = False\n",
    "        if self.width is None and self.height is None:\n",
    "            # infinte board\n",
    "            # let first turn will be on center of the board\n",
    "            # and accessible field will be doubled row\n",
    "            auto_first_turn = True\n",
    "\n",
    "        if self.width is None:\n",
    "            width = self.row * 2 + 1\n",
    "        \n",
    "        if self.height is None:\n",
    "            height = self.row * 2 + 1\n",
    "\n",
    "        self.board = np.full((height, width), Mark.NO.value)\n",
    "        \n",
    "        self.player1 = Player(Mark.X, player1)\n",
    "        self.player2 = Player(Mark.O, player2)\n",
    "\n",
    "        \n",
    "        if auto_first_turn:\n",
    "            self.board[self.row, self.row] = self.player1.mark.value\n",
    "            self.current_player = self.player2\n",
    "        else:\n",
    "            self.current_player = self.player1\n",
    "\n",
    "        self.view = view\n",
    "        if self.view is not None:\n",
    "            self.view.turn_callback = self.apply_action\n",
    "            \n",
    "        if play:\n",
    "            self.start()\n",
    "            \n",
    "    def apply_action(self, row, column):\n",
    "        if self.board[row][column] != Mark.NO.value:\n",
    "            raise RuntimeError('Cell already used')\n",
    "        self.board[row][column] = self.current_player.mark.value\n",
    "        \n",
    "        # check for win\n",
    "        self.winner = self.check_win()\n",
    "        \n",
    "        if self.winner != Mark.NO:\n",
    "            self.end_game = True\n",
    "            # obviously, the winner is current player\n",
    "            self.winner = self.current_player.mark\n",
    "\n",
    "            # distribution of elephants\n",
    "            if self.current_player == self.player1:\n",
    "                self.player1.agent.win(self.new_state())\n",
    "                self.player2.agent.loss(self.new_state())\n",
    "            elif self.current_player == self.player2:\n",
    "                self.player1.agent.loss(self.new_state())\n",
    "                self.player2.agent.win(self.new_state())\n",
    "        \n",
    "        # check for draw\n",
    "        elif len(self.board[self.board != Mark.NO.value]) == 0:\n",
    "            self.end_game = True\n",
    "            self.player1.agent.draw(self.new_state())\n",
    "            self.player2.agent.draw(self.new_state())\n",
    "        else:\n",
    "        # extend board\n",
    "            if self.width is None or self.height is None:\n",
    "                self.extend_board()\n",
    "        \n",
    "            if self.current_player == self.player1:\n",
    "                self.current_player = self.player2\n",
    "            elif self.current_player == self.player2:\n",
    "                self.current_player = self.player1\n",
    "            else:\n",
    "                raise RuntimeError('Unknown player')\n",
    "\n",
    "        self.state = self.new_state()\n",
    "        self.update_view()\n",
    "        \n",
    "        if not self.end_game:\n",
    "            self.get_action()\n",
    "            \n",
    "        \n",
    "    def get_action(self):\n",
    "        action = self.current_player.agent.get_action(self.state)\n",
    "        if action is None:\n",
    "            return # wait callback from UI\n",
    "        else:\n",
    "            row, column = action\n",
    "            self.apply_action(row, column)\n",
    "\n",
    "    def start(self):\n",
    "        self.winner = Mark.NO\n",
    "        self.end_game = False\n",
    "\n",
    "        self.state = self.new_state()\n",
    "        self.update_view()\n",
    "        \n",
    "        self.get_action()\n",
    "        \n",
    "    def new_state(self):\n",
    "        state = GameState(self.board.copy(), self.current_player.mark, self.end_game, self.winner)\n",
    "        return state\n",
    "\n",
    "    def update_view(self):\n",
    "        if self.view is not None:\n",
    "            self.view.update(self.state)\n",
    "            \n",
    "    def check_win(self):\n",
    "        h, w = self.board.shape\n",
    "        \n",
    "        # check rows\n",
    "        for r in range(h):\n",
    "            player = Mark.NO.value\n",
    "            count = 0\n",
    "            for c in range(w):\n",
    "                player, count = self.check_cell(self.board[r, c], player, count)\n",
    "                if count == self.row:\n",
    "                    return player\n",
    "                \n",
    "                    \n",
    "        # check columns\n",
    "        for c in range(w):\n",
    "            player = Mark.NO.value\n",
    "            count = 0\n",
    "            for r in range(h):\n",
    "                player, count = self.check_cell(self.board[r, c], player, count)\n",
    "                if count == self.row:\n",
    "                    return player\n",
    "        \n",
    "        #check diagonal left to right\n",
    "        #top of the board\n",
    "        for cs in range(w - (self.row - 1)):\n",
    "            c = cs\n",
    "            player = Mark.NO.value\n",
    "            count = 0\n",
    "            r = 0\n",
    "            while True:\n",
    "                player, count = self.check_cell(self.board[r, c], player, count)\n",
    "                if count == self.row:\n",
    "                    return player\n",
    "                r += 1\n",
    "                c += 1\n",
    "                if r == h or c == w:\n",
    "                    break\n",
    "                \n",
    "        # bottom of the board\n",
    "        for rs in range(1, h - (self.row - 1)):\n",
    "            r = rs\n",
    "            player = Mark.NO.value\n",
    "            count = 0\n",
    "            c = 0\n",
    "            while True:\n",
    "                player, count = self.check_cell(self.board[r, c], player, count)\n",
    "                if count == self.row:\n",
    "                    return player\n",
    "                r += 1\n",
    "                c += 1\n",
    "                if r == h or c == w:\n",
    "                    break\n",
    "                    \n",
    "        #check diagonal right to left\n",
    "        #top of the board\n",
    "        for cs in range(self.row - 1, w):\n",
    "            c = cs\n",
    "            player = Mark.NO.value\n",
    "            count = 0\n",
    "            r = 0\n",
    "            while True:\n",
    "                player, count = self.check_cell(self.board[r, c], player, count)\n",
    "                if count == self.row:\n",
    "                    return player\n",
    "                c -= 1\n",
    "                r += 1\n",
    "                if c < 0 or r == h:\n",
    "                    break\n",
    "        \n",
    "        # bottom of the board\n",
    "        for rs in range(1, h - self.row + 1):\n",
    "            r = rs\n",
    "            player = Mark.NO.value\n",
    "            count = 0\n",
    "            c = w - 1\n",
    "            while True:\n",
    "                player, count = self.check_cell(self.board[r, c], player, count)\n",
    "                if count == self.row:\n",
    "                    return player\n",
    "                c -= 1\n",
    "                r += 1\n",
    "                if c < 0 or r == h:\n",
    "                    break\n",
    "        \n",
    "        return Mark.NO\n",
    "\n",
    "    def check_cell(self, cell, player, count):\n",
    "        if cell == Mark.NO.value:\n",
    "            player = Mark.NO.value\n",
    "            count = 0\n",
    "        elif cell == player:\n",
    "            count += 1\n",
    "        else:\n",
    "            player = cell\n",
    "            count = 1\n",
    "\n",
    "        return (player, count)\n",
    "\n",
    "    def extend_board(self):\n",
    "        top_rows = 0\n",
    "        for i, r in enumerate(self.board):\n",
    "            if r.sum() > 0:\n",
    "                top_rows = i\n",
    "                break\n",
    "                \n",
    "        bottom_rows = 0\n",
    "        for i, r in enumerate(reversed(self.board)):\n",
    "            if r.sum() > 0:\n",
    "                bottom_rows = i\n",
    "                break\n",
    "                \n",
    "        left_cols = 0\n",
    "        for i, c in enumerate(self.board.T):\n",
    "            if c.sum() > 0:\n",
    "                left_cols = i\n",
    "                break\n",
    "                \n",
    "        right_cols = 0\n",
    "        for i, c in enumerate(reversed(self.board.T)):\n",
    "            if c.sum() > 0:\n",
    "                right_cols = i\n",
    "                break\n",
    "                \n",
    "        add_top = self.row - top_rows\n",
    "        add_bottom = self.row - bottom_rows\n",
    "        add_left = self.row - left_cols\n",
    "        add_right = self.row - right_cols\n",
    "        \n",
    "        top = np.zeros((add_top, self.board.shape[1]))\n",
    "        bottom = np.zeros((add_bottom, self.board.shape[1]))\n",
    "        \n",
    "        left = np.zeros((self.board.shape[0] + add_top + add_bottom, add_left))\n",
    "        right = np.zeros((self.board.shape[0] + add_top + add_bottom, add_right))\n",
    "        \n",
    "        self.board = np.c_[left, np.r_[top, self.board, bottom], right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64b93ad210e4d3881a1db9d2da801d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = Game(player1=JupyterHumanAgent(), player2=JupyterHumanAgent(), view=JupyterView())\n",
    "g.view.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chooser(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def choose_action(self, state, actions):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyChooser(Chooser):\n",
    "    def __init__(self, epsilon, seed=None):\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(seed)\n",
    "            self.random_state = random.getstate()\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def choose_action(self, state, actions):\n",
    "        \"\"\"\n",
    "        actions is list of (action, weight),\n",
    "        \"\"\"\n",
    "            \n",
    "        if self.epsilon > random.random():\n",
    "            # exploration\n",
    "            if self.seed is not None:\n",
    "                random.setstate(self.random_state)\n",
    "                \n",
    "            action = random.choice(actions)\n",
    "            \n",
    "            if self.seed is not None:\n",
    "                self.random_state = random.getstate()\n",
    "        else:\n",
    "            # exploitation\n",
    "            max_weight = np.max([weight for action, weight in actions])\n",
    "            action = random.choice([(action, weight) for action, weight in actions if weight == max_weight])\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningApproximationAgent(Agent):\n",
    "    def __init__(self, alfa, gamma, chooser, row=5, learning=True, weights=None, inf_field=True):\n",
    "        self.alfa = alfa\n",
    "        self.gamma = gamma\n",
    "        self.chooser = chooser\n",
    "        self.is_learning = learning\n",
    "        self.row = row\n",
    "        self.prev_state = None\n",
    "        self.prev_action = None\n",
    "        self.inf_field = inf_field\n",
    "        \n",
    "        \n",
    "        self.features_count = ((((row - 2) * 2) + 1) * 2) - 1\n",
    "        # for example, current player is X, and features will be count of each:\n",
    "        # XXXXX, XXXX , OXXXX , XXX  , OXXX  , XX   , OXX   ,\n",
    "        # OOOO , XOOOO , OOO  , XOOO  , OO   , XOO   .\n",
    "\n",
    "        if weights is not None:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = np.zeros(self.features_count)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        possible_actions = self.get_possible_actions(state)\n",
    "        features = self.get_actions_features(state, possible_actions)\n",
    "        \n",
    "        if self.prev_state is not None and self.is_learning:\n",
    "            revard = self.get_reward(self.prev_state, self.prev_action)\n",
    "            self.td_learn(self.prev_state, self.prev_action, features, reward)\n",
    "        \n",
    "        weighted_actions = self.weigh_action(features)\n",
    "            \n",
    "        action = self.chooser.choose_action(state, weighted_actions)\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action[0]\n",
    "        \n",
    "        return action[0]\n",
    "    \n",
    "    def get_actions_features(self, state, actions):\n",
    "        features = []\n",
    "        for action in possible_actions:\n",
    "            new_state = copy.deepcopy(state)\n",
    "            new_state.board[action[0], action[1]] = new_state.current_player.value\n",
    "            features.append(action, self.get_features(new_state))\n",
    "        return features\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_reward(self, prev_state, prev_action):\n",
    "        if inf_field:\n",
    "            # if board is infinite, real board will grow to infinity too\n",
    "            # so, we need, that the agent move marks to center\n",
    "            h, w = prev_state.board.shape\n",
    "            v = np.where(prev_state.board != 0)\n",
    "            center = np.array([np.mean(v[0]), np.mean(v[1])])\n",
    "            d = np.linalg.norm(np.array([prev_action[0], prev_action[1]]) - center)\n",
    "            if d < 10:\n",
    "                return -1\n",
    "            else:\n",
    "                return np.floor(10 - d)\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        f = np.where(state.board == Mark.NO.value)\n",
    "        possible_actions = list(zip(f[0], f[1]))\n",
    "        return possible_actions\n",
    "    \n",
    "    def get_features(self, state):\n",
    "        directions = [(0, 1), (1, 1), (1, 0), (1, -1)]\n",
    "        b = state.board\n",
    "        h, w = b.shape\n",
    "        #print(f'h, w: {h}, {w}')\n",
    "        features = {}\n",
    "        for i in range(2, self.row):\n",
    "            features[(1, i, True)] = 0\n",
    "            features[(2, i, True)] = 0\n",
    "            features[(1, i, False)] = 0\n",
    "            features[(2, i, False)] = 0\n",
    "        features[(1, self.row, False)] = 0\n",
    "        #rows = []\n",
    "        v = np.zeros((h, w, len(directions)))\n",
    "        \n",
    "        for r in range(h):\n",
    "            for c in range(w):\n",
    "                #print(f'r, c, val: {r}, {c}, {b[r, c]}')\n",
    "                if b[r, c] == Mark.NO.value: # empty cell, next\n",
    "                    continue\n",
    "                for d_i, (d_r, d_c) in enumerate(directions):\n",
    "                    #print(f'direction: {(d_r, d_c, d_i)}')\n",
    "                    if v[r, c, d_i] == 1: # we already count this cell in this direction\n",
    "                        #print('already count')\n",
    "                        continue\n",
    "                    m = b[r, c] # memorize mark\n",
    "                    #row = []\n",
    "                    #row.append((r, c))\n",
    "                    s = 1\n",
    "                    cnt = 1\n",
    "                    enemy_behind = False\n",
    "                    \n",
    "                    # look forward\n",
    "                    nr = r\n",
    "                    nc = c\n",
    "                    steal = True\n",
    "                    for _ in range(self.row):\n",
    "                        nr += d_r\n",
    "                        nc += d_c\n",
    "                        if nr < 0 or nc < 0 or nr == h or nc == w: # beware of borders\n",
    "                            #print('borders')\n",
    "                            break\n",
    "                            \n",
    "                        #if v[nr, nc, d_i] == 1: # perhaps, it is impossible\n",
    "                        #    break\n",
    "                        \n",
    "                        nm = b[nr, nc]\n",
    "                        #print(f'cell: {(nr, nc)}, {nm}')\n",
    "                        #row.append((nr, nc))\n",
    "                        if nm == m: # same mark\n",
    "                            cnt += 1\n",
    "                            v[nr, nc, d_i] = 1\n",
    "                            steal = True\n",
    "                        elif nm != Mark.NO.value: # another mark\n",
    "                            if steal:\n",
    "                                enemy_behind = True\n",
    "                            break\n",
    "                        else:\n",
    "                            steal = False # empty cell\n",
    "                        s += 1\n",
    "                        if s == self.row:\n",
    "                            break\n",
    "                    \n",
    "                    #look backward:\n",
    "                    nr = r\n",
    "                    nc = c\n",
    "                    for i in range(self.row):\n",
    "                        nr -= d_r\n",
    "                        nc -= d_c\n",
    "                        if nr < 0 or nc < 0 or nr == h or nc == w: # beware of borders\n",
    "                            break\n",
    "                            \n",
    "                        nm = b[nr, nc]\n",
    "                        #row.append((nr, nc))\n",
    "                        if nm == m: # same mark, it can't be\n",
    "                            cnt += 1\n",
    "                            v[nr, nc, d_i] = 1\n",
    "                        elif nm != Mark.NO.value: # another mark\n",
    "                            if i == 0:\n",
    "                                enemy_behind = True\n",
    "                            break\n",
    "                        s += 1\n",
    "                        if s >= self.row:\n",
    "                            break\n",
    "                        \n",
    "                    if s < self.row: # between enemies\n",
    "                        continue\n",
    "                    \n",
    "                    if cnt > 1:\n",
    "                        if m == state.current_player.value:\n",
    "                            #rows.append((row, (1, cnt, enemy_behind)))\n",
    "                            features[(1, cnt, enemy_behind)] += 1\n",
    "                        else:\n",
    "                            #rows.append((row, (2, cnt, enemy_behind)))\n",
    "                            features[(2, cnt, enemy_behind)] += 1\n",
    "        #print(rows)\n",
    "        return list(features.values())\n",
    "    \n",
    "    def weigh_actions(self, features):\n",
    "        weighted_actions = []\n",
    "        for action, feat in features:\n",
    "            weighted_actions.append((action, self.get_Q_value(feat)))\n",
    "            \n",
    "        return weighted_actions\n",
    "    \n",
    "    def get_Q_value(self, features):\n",
    "        q_value = np.dot(self.weights, features)\n",
    "        return q_value\n",
    "    \n",
    "    def td_learn(self, prev_state, prev_action, features, reward):\n",
    "        weighted_actions = self.weigh_actions(features)\n",
    "        \n",
    "        max_weight = np.max([weight for action, weight in weighted_actions])\n",
    "        action = random.choice([(action, weight) for action, weight in actions if weight == max_weight])\n",
    "        q_value = action[1]\n",
    "        expected = reward + self.gamma * q_value\n",
    "        \n",
    "        prev_state.board[prev_action[0], prev_action[1]] = prev_state.current_player.value\n",
    "        prev_feat = self.get_features(prev_state)\n",
    "        current = self.get_Q_value(prev_feat)\n",
    "        \n",
    "        td = expected - current\n",
    "        \n",
    "        for i in range(self.features_count):\n",
    "            self.weights[i] += self.alfa * td * prev_feat[i]\n",
    "            \n",
    "    def mc_learn(self, prev_state, prev_action, reward):\n",
    "        prev_state.board[prev_action[0], prev_action[1]] = prev_state.current_player.value\n",
    "        prev_feat = self.get_features(prev_state)\n",
    "    \n",
    "        q_value = self.get_Q_value(prev_feat)\n",
    "        for i in range(self.features_count):\n",
    "            self.weights[i] += self.alfa * (reward - q_value) * prev_feat[i]\n",
    "        \n",
    "    \n",
    "    def loss(self, state):\n",
    "        if self.is_learning:\n",
    "            self.mc_learn(self.prev_state, self.prev_action, -100)\n",
    "        \n",
    "    \n",
    "    def win(self, state):\n",
    "        if self.is_learning:\n",
    "            self.mc_learn(self.prev_state, self.prev_action, 100)\n",
    "        \n",
    "    \n",
    "    def draw(self, state):\n",
    "        if self.is_learning:\n",
    "            self.mc_learn(self.prev_state, self.prev_action, -5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QLearningApproximationAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7a7f3451ad7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_row\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0magent1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearningApproximationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpsilonGreedyChooser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0magent2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearningApproximationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpsilonGreedyChooser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJupyterView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QLearningApproximationAgent' is not defined"
     ]
    }
   ],
   "source": [
    "max_row = 5\n",
    "num_feat = ((((max_row - 2) * 2) + 1) * 2) - 1\n",
    "weights = np.zeros(num_feat)\n",
    "agent1 = QLearningApproximationAgent(0.5, 0.5, EpsilonGreedyChooser(0.05), weights=weights)\n",
    "agent2 = QLearningApproximationAgent(0.5, 0.5, EpsilonGreedyChooser(0.05), weights=weights)\n",
    "g = Game(player1=agent1, player2=agent2, view=JupyterView())\n",
    "g.view.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1d5b73b74143b4a3dca9b426ae7172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_row = 5\n",
    "num_feat = ((((max_row - 2) * 2) + 1) * 2) - 1\n",
    "weights = np.zeros(num_feat)\n",
    "agent1 = QLearningApproximationAgent(0.5, 0.5, EpsilonGreedyChooser(0.05), weights=weights)\n",
    "agent2 = QLearningApproximationAgent(0.5, 0.5, EpsilonGreedyChooser(0.05), weights=weights)\n",
    "g = Game(player1=agent1, player2=JupyterHumanAgent(), view=JupyterView())\n",
    "g.view.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = QLearningApproximationAgent(0.5, 0.5, EpsilonGreedyChooser(0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.get_features(g.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.get_action(g.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.where(g.state.board != 0)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = np.array([np.mean(v[0]), np.mean(v[1])])\n",
    "center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = g.state.board.shape\n",
    "h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = [[0, 0], [0, w-1], [h-1, w-1], [h-1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corners:\n",
    "    print(c)\n",
    "    print(np.linalg.norm(c - center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = np.max([np.linalg.norm(c - center) for c in corners])\n",
    "max_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.zeros((h, w))\n",
    "for r in range(h):\n",
    "    for c in range(w):\n",
    "        d = np.linalg.norm(np.array([r, c]) - center)\n",
    "        if d < 10:\n",
    "            rew = -1\n",
    "        else:\n",
    "            rew = np.floor(10 - d)\n",
    "        reward[r, c] = rew\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([3.0,5.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "* UI переделать на jp_proxy_widget\n",
    "* Выделить апроксимирующую модель в отдельный класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Default python3",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
